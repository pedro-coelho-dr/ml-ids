# MLP Classifier

## Hiperparâmetros Testados

| Hiperparâmetro        | Valores testados              |
|-----------------------|-------------------------------|
| activation            | 'relu', 'tanh'                |
| hidden_layer_sizes    | (100,), (100, 50)             |
| alpha                 | 0.0001, 0.001                 |
| batch_size            | 64, 128                       |

Parâmetros fixos:

- solver: sgd  
- learning_rate_init: 0.3  
- momentum: 0.2  
- n_iter_no_change: 20  
- max_iter: 200  
- early_stopping: True  
- random_state: 1  

## Métricas de Avaliação
As métricas utilizadas foram acurácia e F1-score macro, avaliadas em conjunto de validação separado.

## Resultados (Top 3 por F1-score macro)

| Rank | activation | hidden_layer_sizes | alpha   | batch_size | accuracy  | f1_macro |
|------|------------|--------------------|---------|------------|-----------|----------|
| 1    | tanh       | (100,)             | 0.0001  | 64         | 0.995713  | 0.918910 |
| 2    | tanh       | (100,)             | 0.0001  | 128        | 0.995475  | 0.916376 |
| 3    | tanh       | (100, 50)          | 0.0010  | 64         | 0.996229  | 0.910666 |

## Conclusão
A melhor configuração para maximizar o F1-score macro foi a rede com ativação tanh, camada única (100), alpha 0.0001 e batch_size 64. O uso de uma arquitetura simples, menor regularização e ativação tanh apresentou melhor equilíbrio entre acurácia e sensibilidade às classes minoritárias.
